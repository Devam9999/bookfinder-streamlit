# ðŸ”§ Comprehensive Troubleshooting Guide

This document provides detailed solutions for backend, data pipeline, and infrastructure issues encountered in the **Semantic Book Recommendation System**. It is designed for developers and DevOps engineers maintaining the system.

---

## ðŸ—ï¸ Environment & Installation Issues

### 1. `ModuleNotFoundError: No module named 'sentence_transformers'`
**Symptoms**: The application crashes immediately upon startup with an import error.  
**Cause**: The `sentence-transformers` library is not installed in the current environment, or you are running the script outside the virtual environment.  
**Solution**:
1.  Activate your virtual environment:
    ```bash
    source venv/bin/activate  # Mac/Linux
    venv\Scripts\activate     # Windows
    ```
2.  Install dependencies explicitly:
    ```bash
    pip install -r requirements.txt
    ```
3.  Verify installation:
    ```bash
    python -c "import sentence_transformers; print('Success')"
    ```

### 2. `RuntimeError: PyTorch is not installed`
**Symptoms**: `sentence-transformers` fails to load the model.  
**Cause**: The standard pip install might have failed to find a compatible Torch binary for your OS/Architecture (especially M1/M2 Macs or specific CUDA versions).  
**Solution**:
Install PyTorch manually for your specific architecture:
```bash
# For Mac (Silicon/Intel):
pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu

# For Linux (CUDA 11.8):
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### 3. `SSL: CERTIFICATE_VERIFY_FAILED` during Model Download
**Symptoms**: `requests.exceptions.SSLError` when the pipeline tries to download `all-MiniLM-L6-v2`.  
**Cause**: Corporate firewalls or missing SSL certificates in the Python environment.  
**Solution**:
-   **Option A**: Install local certificates (Mac):
    ```bash
    /Applications/Python\ 3.10/Install\ Certificates.command
    ```
-   **Option B**: Download the model manually and load from local path:
    ```bash
    git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 data/models/all-MiniLM-L6-v2
    ```
    Then update `MODEL_NAME` in `transformation/embedder.py` to point to `data/models/all-MiniLM-L6-v2`.

---

## ðŸ“¡ Data Ingestion & Pipeline Errors

### 4. `HTTP 429: Too Many Requests` (OpenLibrary API)
**Symptoms**: The ingestion script hangs or crashes with a request exception.  
**Cause**: The OpenLibrary API is rate-limiting your IP address due to excessive requests.  
**Solution**:
1.  Increase the sleep interval in `ingestion/openlibrary_loader.py`:
    ```python
    time.sleep(2) # Increase from 1s to 2s
    ```
2.  Run the pipeline with a smaller limit to fetch data in chunks:
    ```bash
    python run_pipeline.py --ingest --limit 10
    ```

### 5. `JSONDecodeError: Expecting value`
**Symptoms**: The pipeline crashes while parsing API responses.  
**Cause**: The API returned a non-JSON response (e.g., 500 Server Error HTML or 502 Bad Gateway).  
**Solution**:
The current ingestion script prints `Error fetching data...`. Check the logs for the specific status code. If the API is down, wait 15 minutes and try again.

---

## ðŸ—„ï¸ Database & Storage Integrity

### 6. `sqlite3.OperationalError: database is locked`
**Symptoms**: "Database is locked" error appears in logs during insertion or search.  
**Cause**: 
-   A previous pipeline run crashed but kept the connection open.
-   You are running `streamlit run app.py` AND `python run_pipeline.py` simultaneously. SQLite allows concurrent reads but only one writer.
**Solution**:
1.  Stop the Streamlit app (`Ctrl+C`).
2.  Stop any running Python scripts.
3.  Restart *only* the pipeline if you are writing data.
4.  Restart the app only after the pipeline finishes.

### 7. `sqlite3.IntegrityError: UNIQUE constraint failed`
**Symptoms**: Logs show skipped records or errors like `UNIQUE constraint failed: books.title, books.author`.  
**Cause**: The system defines a unique index on `(title, author)` to prevent duplicates. This is intentional.  
**Solution**:
-   This is **expected behavior** and handled by `INSERT OR IGNORE`.
-   If you see this as an error (and not a log info), ensure you are using the `insert_books` function which handles this gracefully.

### 8. `PickleError` or `EOFError` loading embeddings
**Symptoms**: `load_embeddings()` fails or crashes.  
**Cause**: The `embeddings.pkl` file is corrupted, empty, or was generated by an incompatible Python version.  
**Solution**:
Delete and regenerate the embeddings:
```bash
rm data/embeddings.pkl
python run_pipeline.py --embed
```

---

## ðŸ§  Embeddings & Search Logic

### 9. Search Returns Unrelated Results
**Symptoms**: Searching for "cooking" returns "war history".  
**Cause**: 
-   Embeddings were generated using a different text format than the search query expects.
-   Normalization was inconsistent (e.g., embeddings normalized but query not).
**Solution**:
Ensure consistency. If you updated the code recently, you MUST regenerate embeddings:
```bash
python run_pipeline.py --embed
```

### 10. `ValueError: shapes (1, 384) and (768, 1) not aligned`
**Symptoms**: Crash during vector similarity calculation.  
**Cause**: The embedding model was changed (e.g., from `all-MiniLM-L6-v2` to `bert-base`) but the `embeddings.pkl` contains vectors from the old model.  
**Solution**:
Deleting `embeddings.pkl` and re-running the embedding phase is mandatory whenever the model changes.

---

## âš¡ Performance Optimization

### 11. High Latency in Search (>500ms)
**Symptoms**: Search feels sluggish.  
**Cause**: 
-   Re-loading the model on every request (if caching is broken).
-   Searching unrelated genres (filtering optimization missing).
-   Large dataset size (>1M records) without FAISS.
**Solution**:
1.  Ensure `@st.cache_resource` is working in `views/home.py`.
2.  The system uses Hard Genre Filtering (`logic` in `semantic_search`) to narrow down candidates. Ensure queries include genre keywords (e.g., "mystery") to trigger this optimization.

---

## ðŸ” Debugging Tools

### Inspecting the Database
You can inspect the SQLite database directly to verify data integrity:
```bash
sqlite3 data/books.db
sqlite> SELECT count(*) FROM books;
sqlite> SELECT title, genre FROM books LIMIT 5;
sqlite> .quit
```

### Verifying Embeddings
Check if embeddings are generated correctly using Python shell:
```python
import pickle
import numpy as np
with open('data/embeddings.pkl', 'rb') as f:
    data = pickle.load(f)
print(f"Count: {len(data['ids'])}")
print(f"Shape: {data['embeddings'].shape}") # Should be (N, 384)
```

---

*For further assistance, please contact the DevOps team or open an issue in the repository.*
